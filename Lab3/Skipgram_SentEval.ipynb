{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(10000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 10 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Druv\\AppData\\Local\\conda\\conda\\envs\\ULL_embed\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "%autosave 10\n",
    "from gensim import models\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method taken from SentEval Examples\n",
    "def get_dictionary(sentences, threshold=0):\n",
    "    words = {}\n",
    "    for s in sentences:\n",
    "        for word in s:\n",
    "            words[word] = words.get(word, 0) + 1\n",
    "\n",
    "    if threshold > 0:\n",
    "        newwords = {}\n",
    "        for word in words:\n",
    "            if words[word] >= threshold:\n",
    "                newwords[word] = words[word]\n",
    "        words = newwords\n",
    "    words['<s>'] = 1e9 + 4\n",
    "    words['</s>'] = 1e9 + 3\n",
    "    words['<p>'] = 1e9 + 2\n",
    "\n",
    "    sorted_words = sorted(words.items(), key=lambda x: -x[1])  # inverse sort\n",
    "    id2word = []\n",
    "    word2id = {}\n",
    "    for i, (w, _) in enumerate(sorted_words):\n",
    "        id2word.append(w)\n",
    "        word2id[w] = i\n",
    "\n",
    "    return id2word, word2id\n",
    "\n",
    "\n",
    "# Skip gram word2vec\n",
    "def get_wordvec(model, word2id):\n",
    "    '''\n",
    "    model: gensim trained skipgram model\n",
    "    word2id: word2index\n",
    "    '''\n",
    "    word_vec = {}\n",
    "    \n",
    "    for word in word2id:\n",
    "        if word in model.wv.vocab:\n",
    "            word_vec[word] = model.wv[word]\n",
    "\n",
    "    return word_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-31 00:42:01,297 : loading Word2Vec object from /Users/Druv/Documents/Jupiter/ULL/Lab3/models/10_30_skip.bin\n",
      "2018-05-31 00:42:01,297 : {'mode': 'rb', 'kw': {}, 'uri': '/Users/Druv/Documents/Jupiter/ULL/Lab3/models/10_30_skip.bin'}\n",
      "2018-05-31 00:42:01,297 : encoding_wrapper: {'errors': 'strict', 'encoding': None, 'mode': 'rb', 'fileobj': <_io.BufferedReader name='/Users/Druv/Documents/Jupiter/ULL/Lab3/models/10_30_skip.bin'>}\n",
      "2018-05-31 00:42:01,890 : loading wv recursively from /Users/Druv/Documents/Jupiter/ULL/Lab3/models/10_30_skip.bin.wv.* with mmap=None\n",
      "2018-05-31 00:42:01,890 : setting ignored attribute vectors_norm to None\n",
      "2018-05-31 00:42:01,890 : loading vocabulary recursively from /Users/Druv/Documents/Jupiter/ULL/Lab3/models/10_30_skip.bin.vocabulary.* with mmap=None\n",
      "2018-05-31 00:42:01,890 : loading trainables recursively from /Users/Druv/Documents/Jupiter/ULL/Lab3/models/10_30_skip.bin.trainables.* with mmap=None\n",
      "2018-05-31 00:42:01,890 : setting ignored attribute cum_table to None\n",
      "2018-05-31 00:42:01,890 : loaded /Users/Druv/Documents/Jupiter/ULL/Lab3/models/10_30_skip.bin\n",
      "2018-05-31 00:42:02,166 : ***** (Probing) Transfer task : LENGTH classification *****\n",
      "2018-05-31 00:42:02,966 : Loaded 99996 train - 9996 dev - 9996 test for Length\n",
      "2018-05-31 00:42:03,767 : Computing embeddings for train/dev/test\n",
      "2018-05-31 00:42:08,515 : Computed embeddings\n",
      "2018-05-31 00:42:08,515 : Training sklearn-LogReg with standard validation..\n",
      "2018-05-31 00:43:14,530 : [('reg:0.25', 21.02), ('reg:0.5', 21.08), ('reg:1', 21.07), ('reg:2', 21.04), ('reg:4', 21.04), ('reg:8', 21.02)]\n",
      "2018-05-31 00:43:14,530 : Validation : best param found is reg = 0.5 with score             21.08\n",
      "2018-05-31 00:43:14,530 : Evaluating...\n",
      "2018-05-31 00:43:25,208 : \n",
      "Dev acc : 21.1 Test acc : 21.9 for LENGTH classification\n",
      "\n",
      "2018-05-31 00:43:25,228 : ***** (Probing) Transfer task : WORDCONTENT classification *****\n",
      "2018-05-31 00:43:25,976 : Loaded 100000 train - 10000 dev - 10000 test for WordContent\n",
      "2018-05-31 00:43:26,890 : Computing embeddings for train/dev/test\n",
      "2018-05-31 00:43:32,415 : Computed embeddings\n",
      "2018-05-31 00:43:32,415 : Training sklearn-LogReg with standard validation..\n",
      "2018-05-31 07:55:03,686 : [('reg:0.25', 64.17), ('reg:0.5', 64.83), ('reg:1', 64.96), ('reg:2', 64.67), ('reg:4', 63.95), ('reg:8', 63.38)]\n",
      "2018-05-31 07:55:03,717 : Validation : best param found is reg = 1 with score             64.96\n",
      "2018-05-31 07:55:03,717 : Evaluating...\n",
      "2018-05-31 08:28:36,424 : \n",
      "Dev acc : 65.0 Test acc : 65.2 for WORDCONTENT classification\n",
      "\n",
      "2018-05-31 08:28:36,549 : ***** (Probing) Transfer task : DEPTH classification *****\n",
      "2018-05-31 08:28:38,064 : Loaded 100000 train - 10000 dev - 10000 test for Depth\n",
      "2018-05-31 08:28:38,861 : Computing embeddings for train/dev/test\n",
      "2018-05-31 08:28:43,189 : Computed embeddings\n",
      "2018-05-31 08:28:43,189 : Training sklearn-LogReg with standard validation..\n",
      "2018-05-31 08:29:47,468 : [('reg:0.25', 21.99), ('reg:0.5', 22.05), ('reg:1', 22.02), ('reg:2', 22.04), ('reg:4', 22.06), ('reg:8', 22.06)]\n",
      "2018-05-31 08:29:47,468 : Validation : best param found is reg = 4 with score             22.06\n",
      "2018-05-31 08:29:47,468 : Evaluating...\n",
      "2018-05-31 08:29:58,281 : \n",
      "Dev acc : 22.1 Test acc : 22.4 for DEPTH classification\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'WordContent': {'ntest': 10000, 'acc': 65.18, 'devacc': 64.96, 'ndev': 10000}, 'Depth': {'ntest': 10000, 'acc': 22.41, 'devacc': 22.06, 'ndev': 10000}, 'Length': {'ntest': 9996, 'acc': 21.87, 'devacc': 21.08, 'ndev': 9996}}\n"
     ]
    }
   ],
   "source": [
    "# Copyright (c) 2017-present, Facebook, Inc.\n",
    "# All rights reserved.\n",
    "#\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "#\n",
    "\n",
    "from __future__ import absolute_import, division, unicode_literals\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import logging\n",
    "import sklearn\n",
    "# import SentEval.examples.data as data\n",
    "\n",
    "# Set PATHs\n",
    "# path to senteval\n",
    "PATH_TO_SENTEVAL = '/Users/Druv/Documents/Jupiter/ULL/Lab3/SentEval/'\n",
    "# path to the NLP datasets \n",
    "PATH_TO_DATA = '/Users/Druv/Documents/Jupiter/ULL/Lab3/SentEval/data'\n",
    "# path to SkipGram model\n",
    "path_to_model = '/Users/Druv/Documents/Jupiter/ULL/Lab3/models/10_30_skip.bin'\n",
    "model = models.Word2Vec.load(path_to_model)\n",
    "\n",
    "# import SentEval\n",
    "sys.path.insert(0, PATH_TO_SENTEVAL)\n",
    "import senteval\n",
    "\n",
    "\n",
    "def prepare(params, samples):\n",
    "    \"\"\"\n",
    "    For Skip gram,\n",
    "    \"\"\"\n",
    "    _, params.word2id = get_dictionary(samples)\n",
    "    # load glove/word2vec format \n",
    "    params.word_vec = get_wordvec(model, params.word2id)\n",
    "    # dimensionality of Skip embeddings\n",
    "    params.wvec_dim = 100\n",
    "    return\n",
    "\n",
    "def batcher(params, batch):\n",
    "    \"\"\"\n",
    "    For Skip gram\n",
    "    \n",
    "    \"\"\"\n",
    "    # if a sentence is empty dot is set to be the only token\n",
    "    batch = [sent if sent != [] else [''] for sent in batch]\n",
    "    embeddings = []\n",
    "\n",
    "    for sent in batch:\n",
    "        sentvec = []\n",
    "        # the format of a sentence is a lists of words (tokenized and lowercased)\n",
    "        for word in sent:\n",
    "            if word in params.word_vec:\n",
    "                # [number of words, embedding dimensionality]\n",
    "                sentvec.append(params.word_vec[word])\n",
    "        if not sentvec:\n",
    "            vec = np.zeros(params.wvec_dim)\n",
    "            # [number of words, embedding dimensionality]\n",
    "            sentvec.append(vec)\n",
    "        # average of word embeddings for sentence representation\n",
    "        # [embedding dimansionality]\n",
    "        sentvec = np.mean(sentvec, 0)\n",
    "        embeddings.append(sentvec)\n",
    "    # [batch size, embedding dimensionality]\n",
    "    embeddings = np.vstack(embeddings)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# Set params for SentEval\n",
    "# we use logistic regression (usepytorch: Fasle) and kfold 10\n",
    "# In this dictionary you can add extra information that you model needs for initialization\n",
    "# for example the path to a dictionary of indices, of hyper parameters\n",
    "# this dictionary is passed to the batched and the prepare fucntions\n",
    "params_senteval = {'task_path': PATH_TO_DATA, 'usepytorch': False, 'kfold': 10}\n",
    "# this is the config for the NN classifier but we are going to use scikit-learn logistic regression with 10 kfold\n",
    "# usepytorch = False \n",
    "#params_senteval['classifier'] = {'nhid': 0, 'optim': 'rmsprop', 'batch_size': 128,\n",
    "#                                 'tenacity': 3, 'epoch_size': 2}\n",
    "\n",
    "# Set up logger\n",
    "logging.basicConfig(format='%(asctime)s : %(message)s', level=logging.DEBUG)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    se = senteval.engine.SE(params_senteval, batcher, prepare)\n",
    "    \n",
    "    # here you define the NLP taks that your embedding model is going to be evaluated\n",
    "    # in (https://arxiv.org/abs/1802.05883) we use the following :\n",
    "    # SICKRelatedness (Sick-R) needs torch cuda to work (even when using logistic regression), \n",
    "    # but STS14 (semantic textual similarity) is a similar type of semantic task\n",
    "    transfer_tasks =  ['Length', 'WordContent', 'Depth']\n",
    "                      # ['TopConstituents','BigramShift', 'Tense',\n",
    "                      #  'SubjNumber', 'ObjNumber', 'OddManOut', 'CoordinationInversion']\n",
    "                      # \n",
    "                      # ['MR', 'CR', 'MPQA', 'SUBJ', 'SST2', 'TREC', 'MRPC', 'SICKEntailment', 'STS14']\n",
    "                      # ['CR', 'MR', 'MPQA', 'SUBJ', 'SST2', 'SST5', 'TREC', 'MRPC', 'SNLI',\n",
    "#                         'SICKEntailment', 'SICKRelatedness', 'STSBenchmark', 'ImageCaptionRetrieval',\n",
    "#                         'STS12', 'STS13', 'STS14', 'STS15', 'STS16',\n",
    "#                         'Length', 'WordContent', 'Depth', 'TopConstituents','BigramShift', 'Tense',\n",
    "#                         'SubjNumber', 'ObjNumber', 'OddManOut', 'CoordinationInversion']\n",
    "    # senteval prints the results and returns a dictionary with the scores\n",
    "    results = se.eval(transfer_tasks)\n",
    "    \n",
    "    with open('30_Skipgram_2.pkl','wb') as f:\n",
    "        pickle.dump(results, f)\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
