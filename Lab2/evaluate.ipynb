{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import scipy.stats as stats\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import normalize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA: True\n"
     ]
    }
   ],
   "source": [
    "CUDA = torch.cuda.is_available()\n",
    "print(\"CUDA: %s\" % CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fld = \"model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def skipgram_sim(embedding, w2i,target, r_ids, context, type=None):\n",
    "    \n",
    "    #target : word to be replaced\n",
    "    #r_ids : replacement words\n",
    "    #context: context words\n",
    "    \n",
    "    t = embedding[target]\n",
    "    r_ids = np.array([embedding[a] for a in r_ids])\n",
    "    \n",
    "    \n",
    "    c_len = len(context)\n",
    "    c = np.array([embedding[w] for w in context])\n",
    "#     print(c.shape, r_ids.shape, t.shape)\n",
    "    c = normalize(c, axis=1, norm='l2')\n",
    "    r_ids = normalize(r_ids, axis=1, norm='l2')\n",
    "    t = t/np.linalg.norm(t)\n",
    "#     print(c.shape, r_ids.shape, t.shape)\n",
    "#     print(r_ids @ t)\n",
    "    if type == 'cosine':\n",
    "            scores = r_ids @ t\n",
    "            scores = scores.tolist()\n",
    "\n",
    "    elif type == 'add':\n",
    "            scores = [\n",
    "                (a @ t + np.sum(c @ a)) / (c_len + 1)\n",
    "            for a\n",
    "            in r_ids\n",
    "    ]\n",
    "    elif type == 'mult':\n",
    "        scores = [\n",
    "                (((t @ a + 1) / 2) * np.prod((c @ a + 1) / 2)) ** (1 / (c_len + 1))\n",
    "                for a\n",
    "                in r_ids\n",
    "        ]\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lexical substituion task\n",
    "\n",
    "\n",
    "def Lst_subs(w2i, win_size = 2):\n",
    "    \n",
    "    #model_type: embedding or whole model\n",
    "    with open('data/lst/lst.gold.candidates', 'r') as f:\n",
    "        lines = map(str.strip, f.readlines())\n",
    "\n",
    "    w2i = defaultdict(lambda: UNK, w2i)\n",
    "    candidates = {}\n",
    "    for line in lines:\n",
    "        target, rest = line.split('.',maxsplit=1)\n",
    "        pos_tag, rest = rest.split('::',maxsplit=1)\n",
    "        replacement = rest.split(';')\n",
    "        candidates[target] = replacement\n",
    "\n",
    "\n",
    "    with open('data/lst/lst_test.preprocessed', 'r') as f:\n",
    "        lines = map(str.strip, f.readlines())\n",
    "\n",
    "    skip = 0\n",
    "#     with open((os.path.join(model_fld,name+'_lst.out')), 'w') as f_out:\n",
    "    t_all = []\n",
    "    r_id_all =[]\n",
    "    c_all =[]\n",
    "    sent_all=[]\n",
    "    rep=[]\n",
    "    target_all=[]\n",
    "    \n",
    "    for line in lines:\n",
    "        target, sent_id, t_pos, sentence = line.split('\\t')\n",
    "\n",
    "        target_word = target.split('.')[0]\n",
    "        sentence = sentence.split()\n",
    "        t_pos = np.int(t_pos)\n",
    "\n",
    "        if target_word in w2i.keys():\n",
    "            target_id = w2i[target_word]\n",
    "        else:\n",
    "            skip+= 1\n",
    "            continue\n",
    "\n",
    "        #get contexts around target positon which are not stopwords and Punctuation\n",
    "        a_context = [w for w in sentence[t_pos+1:] if w.isalpha() if w not in stop_words] \n",
    "        b_context = [w for w in sentence[:t_pos][::-1] if w.isalpha() if w not in stop_words]\n",
    "        context = b_context[:win_size]+a_context[:win_size]\n",
    "\n",
    "        context_ids = [w2i[w] for w in context if w in w2i.keys()]\n",
    "        if len(context_ids)==0:\n",
    "            continue\n",
    "\n",
    "        replacement = candidates[target_word]\n",
    "        r_ids = [w2i[w] for w in replacement if w in w2i.keys()]\n",
    "        t_all.append(target_id)\n",
    "        r_id_all.append(r_ids)\n",
    "        c_all.append(context_ids)\n",
    "        sent_all.append(sent_id)\n",
    "        rep.append(replacement)\n",
    "        target_all.append(target)\n",
    "    return t_all, r_id_all, c_all, sent_all, rep, target_all\n",
    "    \n",
    "#             scores = func(model_type, w2i, target_id, r_ids, context_ids, type=\"kl\")\n",
    "#             break\n",
    "#         #             print(scores)\n",
    "            \n",
    "#             print('RANKED\\t{} {}'.format(target, sent_id), file=f_out, end='')\n",
    "\n",
    "#             # Sort alternative by their scores\n",
    "#             words_and_scores = list(zip(replacement, scores))\n",
    "            \n",
    "#             words_and_scores.sort(key=lambda t: t[1], reverse=True)\n",
    "\n",
    "#             # Write ranked replacement and their scores to file\n",
    "#             for w, s in words_and_scores:\n",
    "#                 print('\\t{} {}'.format(w, s), file=f_out, end='')\n",
    "#             print(file=f_out)\n",
    "#     print(\"done!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SkipGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getout_skip(embedding,target_all, rep_all, context_all, sent_all, rep, word, metrics=\"cosine\"):\n",
    "    \n",
    "    with open((os.path.join(model_fld,\"skip_\"+metrics+'_lst.out')), 'w') as f_out:\n",
    "            for target_id, r_ids, context_ids, sent_id, replacement, target in zip(target_all, rep_all, \\\n",
    "                                                                           context_all, sent_all, rep, word):\n",
    "#                 print(target_id, r_ids, context_ids, sent_id, replacement, target)\n",
    "                scores =  skipgram_sim(embedding, w2i_skip, target_id, r_ids, context_ids, type=metrics)\n",
    "#                 break\n",
    "\n",
    "                print('RANKED\\t{} {}'.format(target, sent_id), file=f_out, end='')\n",
    "\n",
    "                # Sort alternative by their scores\n",
    "                words_and_scores = list(zip(replacement, scores))\n",
    "\n",
    "                words_and_scores.sort(key=lambda t: t[1], reverse=True)\n",
    "\n",
    "                # Write ranked replacement and their scores to file\n",
    "                for w, s in words_and_scores:\n",
    "                    print('\\t{} {}'.format(w, s), file=f_out, end='')\n",
    "                print(file=f_out)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load SkipGram\n",
    "fl = \"skipgram.npz\"\n",
    "with open(os.path.join(model_fld, \"skipgram_w2i.txt\"), 'rb') as f:\n",
    "        w2i_skip = json.load(f)\n",
    "embedding = np.load(os.path.join(model_fld, fl))[\"arr_0\"]\n",
    "target_all, rep_all, context_all, sent_all, rep, word = Lst_subs(w2i_skip, win_size=2)\n",
    "\n",
    "getout_skip(embedding, target_all, rep_all, context_all, sent_all, rep,word,\"cosine\")\n",
    "getout_skip(embedding, target_all, rep_all, context_all, sent_all, rep,word,\"add\")\n",
    "getout_skip(embedding, target_all, rep_all, context_all, sent_all, rep,word,\"mult\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian SkipGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kldiv(mu, sig, mu_rep, sig_rep):\n",
    "    t1 = (mu_rep)*0.5 - mu*0.5\n",
    "    t2_num = (mu-mu_rep)**2 + torch.exp(mu)\n",
    "    t2_den = 2*(torch.exp(sig_rep)) \n",
    "    t2 = ((t2_num/t2_den)-0.5)\n",
    "    kl_loss = t2+t1\n",
    "    kl_loss = torch.sum(kl_loss, dim=1)\n",
    "    return kl_loss.cpu().detach().numpy()[0]\n",
    "    \n",
    "\n",
    "def bsg(model, w2i, target, r_ids, context, win_size = 2,type=None):\n",
    "    #target : word to be replaced\n",
    "    #r_ids : replacement words\n",
    "    #context: context words\n",
    "    \n",
    "    model.eval()\n",
    "    PAD = w2i[\"<pad>\"]\n",
    "    target = [target]\n",
    "    \n",
    "    score =[]\n",
    "    \n",
    "    if len(context)< win_size*2:\n",
    "        context+=[PAD]*(win_size*2-len(context))\n",
    "    #--------------------------------------\n",
    "    #run a fwd pass for target word\n",
    "    if CUDA:\n",
    "        target  = torch.cuda.LongTensor(target)\n",
    "        context = torch.cuda.LongTensor(context).unsqueeze(0)\n",
    "        r_ids   = torch.cuda.LongTensor(r_ids)\n",
    "    else:\n",
    "        target  = torch.LongTensor(target)\n",
    "        context = torch.LongTensor(context).unsqueeze(0)\n",
    "        r_ids   = torch.LongTensor(r_ids)\n",
    "    \n",
    "    \n",
    "    _, mu, sig,_, _ = model.forward(target, context, win_size)\n",
    "    \n",
    "   \n",
    "    #---------------------------------------\n",
    "    context = context.repeat(r_ids.size()[0],1)\n",
    "    \n",
    "    #run fwd pass for each of context word keeping same context\n",
    "    _,mu_r, sig_r, _, _ = model.forward(r_ids, context,win_size)\n",
    "    #----------------------------------------\n",
    "    \n",
    "    #---convert to numpy------\n",
    "    #mu, sig = mu.cpu().detach().numpy(), sig.cpu().detach().numpy()\n",
    "    ##-----------------------------\n",
    "    #mu_r = mu_r.cpu().detach().numpy()\n",
    "   \n",
    "       \n",
    "    #---------------------------\n",
    "    \n",
    "    \n",
    "    if type == \"kl\":\n",
    "        for mu_rep, sig_rep in zip(mu_r, sig_r):\n",
    "            mu_rep = mu_rep.unsqueeze(0)\n",
    "            sig_rep = sig_rep.unsqueeze(0)\n",
    "            \n",
    "            score.append(kldiv(mu, sig, mu_rep, sig_rep))\n",
    "        \n",
    "        return score    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getout_bsg(model, target_all, rep_all, context_all, sent_all, rep, word,metrics=None, win_size=2):\n",
    "    \n",
    "    with open((os.path.join(model_fld,\"bsg_\"+metrics+'_lst.out')), 'w') as f_out:\n",
    "            for target_id, r_ids, context_ids, sent_id, replacement, target in zip(target_all, rep_all, \\\n",
    "                                                                           context_all, sent_all, rep, word):\n",
    "                scores =  bsg(model, w2i_skip, target_id, r_ids, context_ids, win_size, metrics)\n",
    "#                 break\n",
    "\n",
    "                print('RANKED\\t{} {}'.format(target, sent_id), file=f_out, end='')\n",
    "\n",
    "                # Sort alternative by their scores\n",
    "                words_and_scores = list(zip(replacement, scores))\n",
    "\n",
    "                words_and_scores.sort(key=lambda t: t[1], reverse=True)\n",
    "\n",
    "                # Write ranked replacement and their scores to file\n",
    "                for w, s in words_and_scores:\n",
    "                    print('\\t{} {}'.format(w, s), file=f_out, end='')\n",
    "                print(file=f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayesian_skip import BayesianGram\n",
    "\n",
    "fl = \"bsg\"\n",
    "with open(os.path.join(model_fld, \"bsg_w2i.txt\"), 'rb') as f:\n",
    "        w2i_bsg = json.load(f)\n",
    "\n",
    "target_all, rep_all, context_all, sent_all, rep, word = Lst_subs(w2i_skip, win_size=5)\n",
    "model = torch.load(os.path.join(model_fld, fl))\n",
    "\n",
    "getout_bsg(model, target_all, rep_all, context_all, sent_all, rep, word,metrics=\"kl\", win_size = 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EmbedAlign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
