{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import scipy.stats as stats\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import normalize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA: False\n"
     ]
    }
   ],
   "source": [
    "CUDA = torch.cuda.is_available()\n",
    "print(\"CUDA: %s\" % CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fld = \"model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def skipgram_sim(embedding, w2i,target, r_ids, context, type=None):\n",
    "    \n",
    "    #target : word to be replaced\n",
    "    #r_ids : replacement words\n",
    "    #context: context words\n",
    "    \n",
    "    t = embedding[target]\n",
    "    r_ids = np.array([embedding[a] for a in r_ids])\n",
    "    \n",
    "    \n",
    "    c_len = len(context)\n",
    "    c = np.array([embedding[w] for w in context])\n",
    "#     print(c.shape, r_ids.shape, t.shape)\n",
    "    c = normalize(c, axis=1, norm='l2')\n",
    "    r_ids = normalize(r_ids, axis=1, norm='l2')\n",
    "    t = t/np.linalg.norm(t)\n",
    "#     print(c.shape, r_ids.shape, t.shape)\n",
    "#     print(r_ids @ t)\n",
    "    if type == 'cosine':\n",
    "            scores = r_ids @ t\n",
    "            scores = scores.tolist()\n",
    "\n",
    "    elif type == 'add':\n",
    "            scores = [\n",
    "                (a @ t + np.sum(c @ a)) / (c_len + 1)\n",
    "            for a\n",
    "            in r_ids\n",
    "    ]\n",
    "    elif type == 'mult':\n",
    "        scores = [\n",
    "                (((t @ a + 1) / 2) * np.prod((c @ a + 1) / 2)) ** (1 / (c_len + 1))\n",
    "                for a\n",
    "                in r_ids\n",
    "        ]\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lexical substituion task\n",
    "\n",
    "\n",
    "def Lst_subs(w2i, win_size = 2):\n",
    "    \n",
    "    #model_type: embedding or whole model\n",
    "    with open('data/lst/lst.gold.candidates', 'r') as f:\n",
    "        lines = map(str.strip, f.readlines())\n",
    "\n",
    "    w2i = defaultdict(lambda: UNK, w2i)\n",
    "    candidates = {}\n",
    "    for line in lines:\n",
    "        target, rest = line.split('.',maxsplit=1)\n",
    "        pos_tag, rest = rest.split('::',maxsplit=1)\n",
    "        replacement = rest.split(';')\n",
    "        candidates[target] = replacement\n",
    "\n",
    "\n",
    "    with open('data/lst/lst_test.preprocessed', 'r') as f:\n",
    "        lines = map(str.strip, f.readlines())\n",
    "\n",
    "    skip = 0\n",
    "#     with open((os.path.join(model_fld,name+'_lst.out')), 'w') as f_out:\n",
    "    t_all = []\n",
    "    r_id_all =[]\n",
    "    c_all =[]\n",
    "    sent_all=[]\n",
    "    rep=[]\n",
    "    target_all=[]\n",
    "    \n",
    "    for line in lines:\n",
    "        target, sent_id, t_pos, sentence = line.split('\\t')\n",
    "\n",
    "        target_word = target.split('.')[0]\n",
    "        sentence = sentence.split()\n",
    "        t_pos = np.int(t_pos)\n",
    "\n",
    "        if target_word in w2i.keys():\n",
    "            target_id = w2i[target_word]\n",
    "        else:\n",
    "            skip+= 1\n",
    "            continue\n",
    "\n",
    "        #get contexts around target positon which are not stopwords and Punctuation\n",
    "        a_context = [w for w in sentence[t_pos+1:] if w.isalpha() if w not in stop_words] \n",
    "        b_context = [w for w in sentence[:t_pos][::-1] if w.isalpha() if w not in stop_words]\n",
    "        \n",
    "        if win_size != 0:\n",
    "            context = b_context[:win_size]+a_context[:win_size]\n",
    "            context_ids = [w2i[w] for w in context if w in w2i.keys()]\n",
    "        else:\n",
    "            context = [b_context[::-1], a_context]\n",
    "            context_left = [w2i[w] for w in context[0] if w in w2i.keys()]\n",
    "            context_right = [w2i[w] for w in context[1] if w in w2i.keys()]\n",
    "            \n",
    "            if len(context_left)==0 or len(context_right)==0:\n",
    "                continue\n",
    "            context_ids = [context_left, context_right]\n",
    "\n",
    "        \n",
    "        if len(context_ids)==0:\n",
    "            continue\n",
    "\n",
    "        replacement = candidates[target_word]\n",
    "        r_ids = [w2i[w] for w in replacement if w in w2i.keys()]\n",
    "        t_all.append(target_id)\n",
    "        r_id_all.append(r_ids)\n",
    "        c_all.append(context_ids)\n",
    "        sent_all.append(sent_id)\n",
    "        rep.append(replacement)\n",
    "        target_all.append(target)\n",
    "    return t_all, r_id_all, c_all, sent_all, rep, target_all\n",
    "    \n",
    "#             scores = func(model_type, w2i, target_id, r_ids, context_ids, type=\"kl\")\n",
    "#             break\n",
    "#         #             print(scores)\n",
    "            \n",
    "#             print('RANKED\\t{} {}'.format(target, sent_id), file=f_out, end='')\n",
    "\n",
    "#             # Sort alternative by their scores\n",
    "#             words_and_scores = list(zip(replacement, scores))\n",
    "            \n",
    "#             words_and_scores.sort(key=lambda t: t[1], reverse=True)\n",
    "\n",
    "#             # Write ranked replacement and their scores to file\n",
    "#             for w, s in words_and_scores:\n",
    "#                 print('\\t{} {}'.format(w, s), file=f_out, end='')\n",
    "#             print(file=f_out)\n",
    "#     print(\"done!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SkipGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getout_skip(embedding,target_all, rep_all, context_all, sent_all, rep, word, metrics=\"cosine\"):\n",
    "    \n",
    "    with open((os.path.join(model_fld,\"skip_\"+metrics+'_lst.out')), 'w') as f_out:\n",
    "            for target_id, r_ids, context_ids, sent_id, replacement, target in zip(target_all, rep_all, \\\n",
    "                                                                           context_all, sent_all, rep, word):\n",
    "#                 print(target_id, r_ids, context_ids, sent_id, replacement, target)\n",
    "                scores =  skipgram_sim(embedding, w2i_skip, target_id, r_ids, context_ids, type=metrics)\n",
    "#                 break\n",
    "\n",
    "                print('RANKED\\t{} {}'.format(target, sent_id), file=f_out, end='')\n",
    "\n",
    "                # Sort alternative by their scores\n",
    "                words_and_scores = list(zip(replacement, scores))\n",
    "\n",
    "                words_and_scores.sort(key=lambda t: t[1], reverse=True)\n",
    "\n",
    "                # Write ranked replacement and their scores to file\n",
    "                for w, s in words_and_scores:\n",
    "                    print('\\t{} {}'.format(w, s), file=f_out, end='')\n",
    "                print(file=f_out)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load SkipGram\n",
    "fl = \"skipgram.npz\"\n",
    "with open(os.path.join(model_fld, \"skipgram_w2i.txt\"), 'rb') as f:\n",
    "        w2i_skip = json.load(f)\n",
    "embedding = np.load(os.path.join(model_fld, fl))[\"arr_0\"]\n",
    "target_all, rep_all, context_all, sent_all, rep, word = Lst_subs(w2i_skip, win_size=2)\n",
    "\n",
    "getout_skip(embedding, target_all, rep_all, context_all, sent_all, rep,word,\"cosine\")\n",
    "getout_skip(embedding, target_all, rep_all, context_all, sent_all, rep,word,\"add\")\n",
    "getout_skip(embedding, target_all, rep_all, context_all, sent_all, rep,word,\"mult\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian SkipGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kldiv(mu, sig, mu_rep, sig_rep):\n",
    "    t1 = (mu_rep)*0.5 - mu*0.5\n",
    "    t2_num = (mu-mu_rep)**2 + torch.exp(mu)\n",
    "    t2_den = 2*(torch.exp(sig_rep)) \n",
    "    t2 = ((t2_num/t2_den)-0.5)\n",
    "    kl_loss = t2+t1\n",
    "    kl_loss = torch.sum(kl_loss, dim=1)\n",
    "    return kl_loss.cpu().detach().numpy()[0]\n",
    "    \n",
    "\n",
    "def bsg(model, w2i, target, r_ids, context, win_size = 2,type=None):\n",
    "    #target : word to be replaced\n",
    "    #r_ids : replacement words\n",
    "    #context: context words\n",
    "    \n",
    "    model.eval()\n",
    "    PAD = w2i[\"<pad>\"]\n",
    "    target = [target]\n",
    "    \n",
    "    score =[]\n",
    "    \n",
    "    if len(context)< win_size*2:\n",
    "        context+=[PAD]*(win_size*2-len(context))\n",
    "    #--------------------------------------\n",
    "    #run a fwd pass for target word\n",
    "    if CUDA:\n",
    "        target  = torch.cuda.LongTensor(target)\n",
    "        context = torch.cuda.LongTensor(context).unsqueeze(0)\n",
    "        r_ids   = torch.cuda.LongTensor(r_ids)\n",
    "    else:\n",
    "        target  = torch.LongTensor(target)\n",
    "        context = torch.LongTensor(context).unsqueeze(0)\n",
    "        r_ids   = torch.LongTensor(r_ids)\n",
    "    \n",
    "    \n",
    "    _, mu, sig,_, _ = model.forward(target, context, win_size)\n",
    "    \n",
    "   \n",
    "    #---------------------------------------\n",
    "    context = context.repeat(r_ids.size()[0],1)\n",
    "    \n",
    "    #run fwd pass for each of context word keeping same context\n",
    "    _,mu_r, sig_r, _, _ = model.forward(r_ids, context,win_size)\n",
    "    #----------------------------------------\n",
    "    \n",
    "    #---convert to numpy------\n",
    "    #mu, sig = mu.cpu().detach().numpy(), sig.cpu().detach().numpy()\n",
    "    ##-----------------------------\n",
    "    #mu_r = mu_r.cpu().detach().numpy()\n",
    "   \n",
    "       \n",
    "    #---------------------------\n",
    "    \n",
    "    \n",
    "    if type == \"kl\":\n",
    "        for mu_rep, sig_rep in zip(mu_r, sig_r):\n",
    "            mu_rep = mu_rep.unsqueeze(0)\n",
    "            sig_rep = sig_rep.unsqueeze(0)\n",
    "            \n",
    "            score.append(kldiv(mu, sig, mu_rep, sig_rep))\n",
    "        \n",
    "        return score    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getout_bsg(model, target_all, rep_all, context_all, sent_all, rep, word,metrics=None, win_size=2):\n",
    "    \n",
    "    with open((os.path.join(model_fld,\"bsg_\"+metrics+'_lst.out')), 'w') as f_out:\n",
    "            for target_id, r_ids, context_ids, sent_id, replacement, target in zip(target_all, rep_all, \\\n",
    "                                                                           context_all, sent_all, rep, word):\n",
    "                scores =  bsg(model, w2i_skip, target_id, r_ids, context_ids, win_size, metrics)\n",
    "#                 break\n",
    "\n",
    "                print('RANKED\\t{} {}'.format(target, sent_id), file=f_out, end='')\n",
    "\n",
    "                # Sort alternative by their scores\n",
    "                words_and_scores = list(zip(replacement, scores))\n",
    "\n",
    "                words_and_scores.sort(key=lambda t: t[1], reverse=True)\n",
    "\n",
    "                # Write ranked replacement and their scores to file\n",
    "                for w, s in words_and_scores:\n",
    "                    print('\\t{} {}'.format(w, s), file=f_out, end='')\n",
    "                print(file=f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayesian_skip import BayesianGram\n",
    "\n",
    "fl = \"bsg\"\n",
    "with open(os.path.join(model_fld, \"bsg_w2i.txt\"), 'rb') as f:\n",
    "        w2i_bsg = json.load(f)\n",
    "\n",
    "target_all, rep_all, context_all, sent_all, rep, word = Lst_subs(w2i_skip, win_size=5)\n",
    "model = torch.load(os.path.join(model_fld, fl))\n",
    "\n",
    "getout_bsg(model, target_all, rep_all, context_all, sent_all, rep, word,metrics=\"kl\", win_size = 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EmbedAlign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(words,context,sentence_length, batch_size, pad):\n",
    "    target = words[0]\n",
    "    r_ids = words[1]\n",
    "    \n",
    "    batch = torch.zeros(batch_size,sentence_length)\n",
    "    \n",
    "    for i in range(0,len(r_ids)+1):\n",
    "        if i == 0:\n",
    "            sentence = context[0] + [target] + context[1]\n",
    "        else:\n",
    "            sentence = context[0] + [r_ids[i-1]] + context[1]\n",
    "        \n",
    "        if len(sentence) < sentence_length:\n",
    "            diff = sentence_length - len(sentence)\n",
    "            pad_m = [pad]*diff\n",
    "            sentence_add = sentence + pad_m\n",
    "        else:\n",
    "            sentence_add = sentence\n",
    "\n",
    "        batch[i,:] = torch.Tensor(sentence_add)\n",
    "        \n",
    "    return batch.long()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target = w2i, r_id=w2i, sentence=:::\n",
    "def embedalign(model, w2i_embdL1, target, r_ids, context):\n",
    "    hidden_dim = 100 # Use the same values as training\n",
    "    batch_size = 128  # Use the same values as training\n",
    "    sentence_length = 64 # Could use the same as training\n",
    "    dim_Z = 150\n",
    "    \n",
    "    \n",
    "    ffnn1 = models[0]\n",
    "    ffnn2 = models[1]\n",
    "    ffnn3 = models[2]\n",
    "    ffnn4 = models[3]\n",
    "    lstm = models[4]\n",
    "\n",
    "    ffnn1.eval()\n",
    "    ffnn2.eval()\n",
    "    ffnn3.eval()\n",
    "    ffnn4.eval()\n",
    "    lstm.eval()\n",
    "    \n",
    "    \n",
    "    words = [target, r_ids]\n",
    "    pad = w2i_embdL1[\"<pad>\"]\n",
    "    \n",
    "    data = get_batch(words,context,sentence_length, batch_size, pad)\n",
    "    lstm.word_embeddings.eval()\n",
    "  \n",
    "    if data.shape[0] != batch_size:\n",
    "        required_size = batch_size - data.shape[0]\n",
    "        dummy_data = torch.zeros(required_size,sentences)\n",
    "        print(dummy_data.shape)\n",
    "        data = torch.cat(data, dummy_data)\n",
    "            \n",
    "\n",
    "    # Because training was done in batch, the LSTM structure/model seems to be hardcoded for batch\n",
    "    h_1 = lstm(data)\n",
    "    h = (h_1[:, :, 0:hidden_dim] + h_1[:, :, hidden_dim:]) / 2\n",
    "\n",
    "    mu = ffnn3(h, linear_activation=True)\n",
    "    sigma = ffnn4(h)\n",
    "\n",
    "    return mu, sigma\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getout_embed(models, target_all, rep_all, context_all, sent_all, rep, word,metrics=None, win_size=0):\n",
    "\n",
    "    with open((os.path.join(model_fld,\"embed_\"+metrics+'_lst.out')), 'w') as f_out:\n",
    "            for target_id, r_ids, context_ids, sent_id, replacement, target in zip(target_all, rep_all, \\\n",
    "                                                                           context_all, sent_all, rep, word):\n",
    "                mu, sigma =  embedalign(models, w2i_embdL1, target_id, r_ids, context_ids)\n",
    "\n",
    "                target_pos = len(context_ids[0])\n",
    "                mu_t = mu[0,target_pos,:]\n",
    "                sigma_t = sigma[0,target_pos,:]\n",
    "                scores = []\n",
    "                for i in range(0,mu.shape[0]):\n",
    "                    if i == 0:\n",
    "                        continue\n",
    "                    else:\n",
    "                        mu_i = mu[i,target_pos,:]\n",
    "                        sigma_i = sigma[i,target_pos,:]\n",
    "                        \n",
    "                    scores.append(kldiv(mu_t.unsqueeze(0), torch.log(sigma_t.unsqueeze(0)), mu_i.unsqueeze(0), \\\n",
    "                                        torch.log(sigma_i.unsqueeze(0))))\n",
    "                    \n",
    "                print('RANKED\\t{} {}'.format(target, sent_id), file=f_out, end='')\n",
    "\n",
    "                # Sort alternative by their scores\n",
    "                words_and_scores = list(zip(replacement, scores))\n",
    "\n",
    "                words_and_scores.sort(key=lambda t: t[1], reverse=True)\n",
    "\n",
    "                # Write ranked replacement and their scores to file\n",
    "                for w, s in words_and_scores:\n",
    "                    print('\\t{} {}'.format(w, s), file=f_out, end='')\n",
    "                print(file=f_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models():\n",
    "    ffnn1 = torch.load('ffnn1.pt')\n",
    "    ffnn2 = torch.load('ffnn2.pt')\n",
    "    ffnn3 = torch.load('ffnn3.pt')\n",
    "    ffnn4 = torch.load('ffnn4.pt')\n",
    "\n",
    "    lstm = torch.load('ffnn5.pt')\n",
    "\n",
    "    return [ffnn1, ffnn2, ffnn3, ffnn4, lstm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_name = \"L1_w2i_f.json\"\n",
    "with open(os.path.join(model_fld,file_name), 'rb') as f:\n",
    "    w2i_embdL1 = json.load(f)\n",
    "       \n",
    "target_all, rep_all, context_all, sent_all, rep, word = Lst_subs(w2i_embdL1, win_size=0)\n",
    "\n",
    "models = load_models()\n",
    "\n",
    "# This will freeze and consume all the memory\n",
    "getout_embed(models, target_all, rep_all, context_all, sent_all, rep, word,metrics=\"kl\", win_size = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
